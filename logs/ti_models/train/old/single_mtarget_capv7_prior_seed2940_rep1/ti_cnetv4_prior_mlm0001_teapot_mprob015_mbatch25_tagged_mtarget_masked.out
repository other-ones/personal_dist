INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+cu118)
    Python  3.9.18 (you have 3.9.19)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
wandb: Currently logged in as: tuk101234 (qlab-taewook). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/twkim/project/rich_context/textual_inversion/wandb/run-20240911_050432-51km5vqm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-salad-574
wandb: â­ï¸ View project at https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE
wandb: ğŸš€ View run at https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE/runs/51km5vqm
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
INFO:__main__:Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'timestep_spacing', 'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'variance_type', 'prediction_type', 'thresholding'} was not found in config. Values will be initialized to default values.
{'scaling_factor'} was not found in config. Values will be initialized to default values.
{'time_embedding_type', 'encoder_hid_dim_type', 'mid_block_only_cross_attention', 'addition_embed_type_num_heads', 'resnet_skip_time_act', 'addition_time_embed_dim', 'addition_embed_type', 'conv_out_kernel', 'conv_in_kernel', 'upcast_attention', 'class_embeddings_concat', 'encoder_hid_dim', 'transformer_layers_per_block', 'num_attention_heads', 'num_class_embeds', 'dual_cross_attention', 'use_linear_projection', 'only_cross_attention', 'resnet_time_scale_shift', 'time_cond_proj_dim', 'mid_block_type', 'projection_class_embeddings_input_dim', 'time_embedding_dim', 'resnet_out_scale_factor', 'class_embed_type', 'timestep_post_act', 'time_embedding_act_fn', 'cross_attention_norm'} was not found in config. Values will be initialized to default values.
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 10000000
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4
INFO:__main__:  Gradient Accumulation steps = 4
INFO:__main__:  Total optimization steps = 3001
set seed 2940
saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/teapot/ti_cnetv4_prior_mlm0001_teapot_mprob015_mbatch25_tagged_mtarget_masked/src/command.txt command_path
ti_train.py item
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 item
--train_data_dir1=/data/twkim/diffusion/personalization/collected/images/teapot item
--learnable_property=object item
--placeholder_token1=<teapot> item
--train_prior_concept1=teapot item
--eval_prior_concept1=teapot item
--resolution=512 item
--train_batch_size=1 item
--gradient_accumulation_steps=4 item
--max_train_steps=3001 item
--learning_rate=5e-4 item
--lr_scheduler=constant item
--initializer_token=teapot item
--normalize_mask_embeds=0 item
--lr_warmup_steps=0 item
--output_dir=saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/teapot item
--seed=2940 item
--mask_tokens=[MASK] item
--lambda_mlm=0.001 item
--freeze_mask_embedding=1 item
--cls_net_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/cls_net_100000_ckpt.pt item
--mask_embed_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/mask_embeds_100000_ckpt.pt item
--mask_prob=0.15 item
--mlm_batch_size=25 item
--scale_lr item
--eval_prompt_type=nonliving item
--train_prompt_type=nonliving item
--silent=0 item
--rev=0 item
--check_tag=VERB-ADJ-ADV-PROPN-ADP-NOUN item
--mlm_target=masked item
--normalize_target1=0 item
--caption_root=../datasets_pkgs/captions/v7 item
--run_name=ti_cnetv4_prior_mlm0001_teapot_mprob015_mbatch25_tagged_mtarget_masked item
--report_to=wandb item
--project_name=TI MLM SINGLE item
--include_prior_concept=1 item
32 norm_num_groups
320 block_out_channels[0]
32 norm_num_groups
text_model.embeddings.token_embedding.weight text_encoder requires
[49409] mask_token_ids
seeded
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
captions_nonliving_human_interactions	4176
captions_nonliving_relations	29376
captions_nonliving_styles	378
seeded
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
captions_nonliving_human_interactions	4176
captions_nonliving_relations	29376
captions_nonliving_styles	378
position_ids defined_key-clsnet
final.weight defined_key-clsnet
final.bias defined_key-clsnet
position_embedding.weight defined_key-clsnet
multi_head_attention1.in_proj_weight defined_key-clsnet
multi_head_attention1.in_proj_bias defined_key-clsnet
multi_head_attention1.out_proj.weight defined_key-clsnet
multi_head_attention1.out_proj.bias defined_key-clsnet
feed_forward1.W_ff1.weight defined_key-clsnet
feed_forward1.W_ff1.bias defined_key-clsnet
feed_forward1.W_ff2.weight defined_key-clsnet
feed_forward1.W_ff2.bias defined_key-clsnet
layer_norm1.weight defined_key-clsnet
layer_norm1.bias defined_key-clsnet
multi_head_attention2.in_proj_weight defined_key-clsnet
multi_head_attention2.in_proj_bias defined_key-clsnet
multi_head_attention2.out_proj.weight defined_key-clsnet
multi_head_attention2.out_proj.bias defined_key-clsnet
layer_norm2.weight defined_key-clsnet
layer_norm2.bias defined_key-clsnet

module.position_ids saved_key-clsnet
module.final.weight saved_key-clsnet
module.final.bias saved_key-clsnet
module.position_embedding.weight saved_key-clsnet
module.multi_head_attention1.in_proj_weight saved_key-clsnet
module.multi_head_attention1.in_proj_bias saved_key-clsnet
module.multi_head_attention1.out_proj.weight saved_key-clsnet
module.multi_head_attention1.out_proj.bias saved_key-clsnet
module.feed_forward1.W_ff1.weight saved_key-clsnet
module.feed_forward1.W_ff1.bias saved_key-clsnet
module.feed_forward1.W_ff2.weight saved_key-clsnet
module.feed_forward1.W_ff2.bias saved_key-clsnet
module.layer_norm1.weight saved_key-clsnet
module.layer_norm1.bias saved_key-clsnet
module.multi_head_attention2.in_proj_weight saved_key-clsnet
module.multi_head_attention2.in_proj_bias saved_key-clsnet
module.multi_head_attention2.outSteps:   0% 0/3001 [00:00<?, ?it/s]_proj.weight saved_key-clsnet
module.multi_head_attention2.out_proj.bias saved_key-clsnet
module.layer_norm2.weight saved_key-clsnet
module.layer_norm2.bias saved_key-clsnet
True accepts_keep_fp32_wrapper
{'keep_fp32_wrapper': True} extra_args
INFO:__main__:STEP 0 Running validation... 
 Generating 8 images with prompt: ['a <teapot> teapot in the jungle', 'a <teapot> teapot with a city in the background', 'a <teapot> teapot with a mountain in the background', 'a <teapot> teapot with the Eiffel Tower in the background', 'a <teapot> teapot floating on top of water', 'a <teapot> teapot floating in an ocean of milk', 'a <teapot> teapot on top of the sidewalk in a crowded street', 'a cube shaped <teapot> teapot'].

  0% 0/25 [00:00<?, ?it/s][A

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Step		|0
Raw		|vincent    van        gogh       is         auc        M[tioning] <teapot>   teapot    
Masked		|vincent    van        gogh       is         auc        [MASK]     <teapot>   teapot    
Preds		|an         c          gogh       is         sure       M[able]    teapot     teapot    
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------


  4% 1/25 [00:00<00:09,  2.58it/s][A
  8% 2/25 [00:00<00:08,  2.79it/s][A
 12% 3/25 [00:01<00:07,  2.86it/s][A
 16% 4/25 [00:01<00:07,  2.90it/s][A
 20% 5/25 [00:01<00:06,  2.92it/s][A
 24% 6/25 [00:02<00:06,  2.94it/s][A
 28% 7/25 [00:02<00:06,  2.94it/s][A
 32% 8/25 [00:02<00:05,  2.95it/s][A
 36% 9/25 [00:03<00:05,  2.95it/s][A
 40% 10/25 [00:03<00:05,  2.96it/s][A
 44% 11/25 [00:03<00:04,  2.96it/s][A
 48% 12/25 [00:04<00:04,  2.96it/s][A
 52% 13/25 [00:04<00:04,  2.96it/s][A
 56% 14/25 [00:04<00:03,  2.96it/s][A
 60% 15/25 [00:05<00:03,  2.96it/s][A
 64% 16/25 [00:05<00:03,  2.96it/s][A
 68% 17/25 [00:05<00:02,  2.96it/s][A
 72% 18/25 [00:06<00:02,  2.96it/s][A
 76% 19/25 [00:06<00:02,  2.96it/s][A
 80% 20/25 [00:06<00:01,  2.96it/s][A
 84% 21/25 [00:07<00:01,  2.96it/s][A
 88% 22/25 [00:07<00:01,  2.95it/s][A
 92% 23/25 [00:07<00:00,  2.95it/s][A
 96% 24/25 [00:08<00:00,  2.95it/s][A
100% 25/25 [00:08<00:00,  2.95it/s][A100% 25/25 [00:09<00:00,  2.67it/s]
Steps:   0% 0/3001 [00:20<?, ?it/s, _runtime=31, _timestamp=1.73e+9, loss=0.179, loss_mlm=0.0959, lr=0.002, norm_mask=8.02, norm_target=8.42]Steps:   0% 1/3001 [00:20<16:45:10, 20.10s/it, _runtime=31, _timestamp=1.73e+9, loss=0.179, loss_mlm=0.0959, lr=0.002, norm_mask=8.02, norm_target=8.42]Steps:   0% 1/3001 [00:21<16:45:10, 20.10s/it, _runtime=32, _timestamp=1.73e+9, loss=0.548, loss_mlm=0.0743, lr=0.002, norm_mask=8.02, norm_target=8.42]Steps:   0% 2/3001 [00:21<7:30:27,  9.01s/it, _runtime=32, _timestamp=1.73e+9, loss=0.548, loss_mlm=0.0743, lr=0.002, norm_mask=8.02, norm_target=8.42] Steps:   0% 2/3001 [00:22<7:30:27,  9.01s/it, _runtime=34, _timestamp=1.73e+9, loss=0.103, loss_mlm=0.0997, lr=0.002, norm_mask=8.02, norm_target=8.42]Steps:   0% 3/3001 [00:22<4:31:32,  5.43s/it, _runtime=34, _timestamp=1.73e+9, loss=0.103, loss_mlm=0.0997, lr=0.002, norm_mask=8.02, norm_target=8.42]Steps:   0% 3/3001 [00:23<4:31:32,  5.43s/it, _runtime=35, _timestamp=1.73e+9, loss=0.00436, loss_mlm=0.0422, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 4/3001 [00:23<3:08:08,  3.77s/it, _runtime=35, _timestamp=1.73e+9, loss=0.00436, loss_mlm=0.0422, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 4/3001 [00:24<3:08:08,  3.77s/it, _runtime=36, _timestamp=1.73e+9, loss=0.00352, loss_mlm=0.0951, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 5/3001 [00:24<2:21:43,  2.84s/it, _runtime=36, _timestamp=1.73e+9, loss=0.00352, loss_mlm=0.0951, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 5/3001 [00:26<2:21:43,  2.84s/it, _runtime=37, _timestamp=1.73e+9, loss=0.00479, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 6/3001 [00:26<1:53:37,  2.28s/it, _runtime=37, _timestamp=1.73e+9, loss=0.00479, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 6/3001 [00:27<1:53:37,  2.28s/it, _runtime=38, _timestamp=1.73e+9, loss=0.0245, loss_mlm=0.0591, lr=0.002, norm_mask=8.02, norm_target=8.59] Steps:   0% 7/3001 [00:27<1:35:56,  1.92s/it, _runtime=38, _timestamp=1.73e+9, loss=0.0245, loss_mlm=0.0591, lr=0.002, norm_mask=8.02, norm_target=8.59]Steps:   0% 7/3001 [00:28<1:35:56,  1.92s/it, _runtime=39, _timestamp=1.73e+9, loss=0.143, loss_mlm=0.0923, lr=0.002, norm_mask=8.02, norm_target=8.72] Steps:   0% 8/3001 [00:28<1:24:12,  1.69s/it, _runtime=39, _timestamp=1.73e+9, loss=0.143, loss_mlm=0.0923, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 8/3001 [00:29<1:24:12,  1.69s/it, _runtime=41, _timestamp=1.73e+9, loss=0.269, loss_mlm=0.0387, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 9/3001 [00:29<1:16:18,  1.53s/it, _runtime=41, _timestamp=1.73e+9, loss=0.269, loss_mlm=0.0387, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 9/3001 [00:30<1:16:18,  1.53s/it, _runtime=42, _timestamp=1.73e+9, loss=0.181, loss_mlm=0.0816, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 10/3001 [00:30<1:11:08,  1.43s/it, _runtime=42, _timestamp=1.73e+9, loss=0.181, loss_mlm=0.0816, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 10/3001 [00:32<1:11:08,  1.43s/it, _runtime=43, _timestamp=1.73e+9, loss=0.0187, loss_mlm=0.0475, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 11/3001 [00:32<1:07:28,  1.35s/it, _runtime=43, _timestamp=1.73e+9, loss=0.0187, loss_mlm=0.0475, lr=0.002, norm_mask=8.02, norm_target=8.72]Steps:   0% 11/3001 [00:33<1:07:28,  1.35s/it, _runtime=44, _timestamp=1.73e+9, loss=0.332, loss_mlm=0.113, lr=0.002, norm_mask=8.02, norm_target=8.86]  Steps:   0% 12/3001 [00:33<1:04:55,  1.30s/it, _runtime=44, _timestamp=1.73e+9, loss=0.332, loss_mlm=0.113, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 12/3001 [00:34<1:04:55,  1.30s/it, _runtime=45, _timestamp=1.73e+9, loss=0.0851, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 13/3001 [00:34<1:03:02,  1.27s/it, _runtime=45, _timestamp=1.73e+9, loss=0.0851, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 13/3001 [00:35<1:03:02,  1.27s/it, _runtime=47, _timestamp=1.73e+9, loss=0.0749, loss_mlm=0.0729, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 14/3001 [00:35<1:02:06,  1.25s/it, _runtime=47, _timestamp=1.73e+9, loss=0.0749, loss_mlm=0.0729, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 14/3001 [00:36<1:02:06,  1.25s/it, _runtime=48, _timestamp=1.73e+9, loss=0.016, loss_mlm=0.0738, lr=0.002, norm_mask=8.02, norm_target=8.86] Steps:   0% 15/3001 [00:36<1:01:25,  1.23s/it, _runtime=48, _timestamp=1.73e+9, loss=0.016, loss_mlm=0.0738, lr=0.002, norm_mask=8.02, norm_target=8.86]Steps:   0% 15/3001 [00:38<1:01:25,  1.23s/it, _runtime=49, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.0652, lr=0.002, norm_mask=8.02, norm_target=9]  Steps:   1% 16/3001 [00:38<1:00:54,  1.22s/it, _runtime=49, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.0652, lr=0.002, norm_mask=8.02, norm_target=9]Steps:   1% 16/3001 [00:39<1:00:54,  1.22s/it, _runtime=50, _timestamp=1.73e+9, loss=0.0156, loss_mlm=0.0562, lr=0.002, norm_mask=8.02, norm_target=9]Steps:   1% 17/3001 [00:39<1:00:27,  1.22s/it, _runtime=50, _timestamp=1.73e+9, loss=0.0156, loss_mlm=0.0562, lr=0.002, norm_mask=8.02, norm_target=9]Steps:   1% 17/3001 [00:40<1:00:27,  1.22s/it, _runtime=51, _timestamp=1.73e+9, loss=0.246, loss_mlm=0.0926, lr=0.002, norm_mask=8.02, norm_target=9] Steps:   1% 18/3001 [00:40<1:00:12,  1.21s/it, _runtime=51, _timestamp=1.73e+9, loss=0.246, loss_mlm=0.0926, lr=0.002, norm_mask=8.02, norm_target=9]Steps:   1% 18/3001 [00:41<1:00:12,  1.21s/it, _runtime=53, _timestamp=1.73e+9, loss=0.0189, loss_mlm=0.0714, lr=0.002, norm_mask=8.02, norm_target=9]Steps:   1% 19/3001 [00:41<59:46,  1.20s/it, _runtime=53, _timestamp=1.73e+9, loss=0.0189, loss_mlm=0.0714, lr=0.002, norm_mask=8.02, norm_target=9]  Steps:   1% 19/3001 [00:43<59:46,  1.20s/it, _runtime=54, _timestamp=1.73e+9, loss=0.00237, loss_mlm=0.0264, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 20/3001 [00:43<1:05:08,  1.31s/it, _runtime=54, _timestamp=1.73e+9, loss=0.00237, loss_mlm=0.0264, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 20/3001 [00:44<1:05:08,  1.31s/it, _runtime=55, _timestamp=1.73e+9, loss=0.119, loss_mlm=0.056, lr=0.002, norm_mask=8.02, norm_target=9.17]   Steps:   1% 21/3001 [00:44<1:03:12,  1.27s/it, _runtime=55, _timestamp=1.73e+9, loss=0.119, loss_mlm=0.056, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 21/3001 [00:45<1:03:12,  1.27s/it, _runtime=57, _timestamp=1.73e+9, loss=0.237, loss_mlm=0.0725, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 22/3001 [00:45<1:01:52,  1.25s/it, _runtime=57, _timestamp=1.73e+9, loss=0.237, loss_mlm=0.0725, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 22/3001 [00:46<1:01:52,  1.25s/it, _runtime=58, _timestamp=1.73e+9, loss=0.0675, loss_mlm=0.0448, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 23/3001 [00:46<1:00:51,  1.23s/it, _runtime=58, _timestamp=1.73e+9, loss=0.0675, loss_mlm=0.0448, lr=0.002, norm_mask=8.02, norm_target=9.17]Steps:   1% 23/3001 [00:47<1:00:51,  1.23s/it, _runtime=59, _timestamp=1.73e+9, loss=0.101, loss_mlm=0.024, lr=0.002, norm_mask=8.02, norm_target=9.35]  Steps:   1% 24/3001 [00:47<1:00:17,  1.22s/it, _runtime=59, _timestamp=1.73e+9, loss=0.101, loss_mlm=0.024, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 24/3001 [00:49<1:00:17,  1.22s/it, _runtime=60, _timestamp=1.73e+9, loss=0.144, loss_mlm=0.0879, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 25/3001 [00:49<59:45,  1.20s/it, _runtime=60, _timestamp=1.73e+9, loss=0.144, loss_mlm=0.0879, lr=0.002, norm_mask=8.02, norm_target=9.35]  Steps:   1% 25/3001 [00:50<59:45,  1.20s/it, _runtime=61, _timestamp=1.73e+9, loss=0.0102, loss_mlm=0.0956, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 26/3001 [00:50<59:39,  1.20s/it, _runtime=61, _timestamp=1.73e+9, loss=0.0102, loss_mlm=0.0956, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 26/3001 [00:51<59:39,  1.20s/it, _runtime=62, _timestamp=1.73e+9, loss=0.0148, loss_mlm=0.0856, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 27/3001 [00:51<59:23,  1.20s/it, _runtime=62, _timestamp=1.73e+9, loss=0.0148, loss_mlm=0.0856, lr=0.002, norm_mask=8.02, norm_target=9.35]Steps:   1% 27/3001 [00:52<59:23,  1.20s/it, _runtime=64, _timestamp=1.73e+9, loss=0.0204, loss_mlm=0.0702, lr=0.002, norm_mask=8.02, norm_target=9.5] Steps:   1% 28/3001 [00:52<59:10,  1.19s/it, _runtime=64, _timestamp=1.73e+9, loss=0.0204, loss_mlm=0.0702, lr=0.002, norm_mask=8.02, norm_target=9.5]Steps:   1% 28/3001 [00:53<59:10,  1.19s/it, _runtime=65, _timestamp=1.73e+9, loss=0.177, loss_mlm=0.0743, lr=0.002, norm_mask=8.02, norm_target=9.5] Steps:   1% 29/3001 [00:53<59:04,  1.19s/it, _runtime=65, _timestamp=1.73e+9, loss=0.177, loss_mlm=0.0743, lr=0.002, norm_mask=8.02, norm_target=9.5]Steps:   1% 29/3001 [00:55<59:04,  1.19s/it, _runtime=66, _timestamp=1.73e+9, loss=0.0789, loss_mlm=0.0541, lr=0.002, norm_mask=8.02, norm_target=9.5]Steps:   1% 30/3001 [00:55<59:04,  1.19s/it, _runtime=66, _timestamp=1.73e+9, loss=0.0789, loss_mlm=0.0541, lr=0.002, norm_mask=8.02, norm_target=9.5]439510308 worker_seed
439510306 worker_seed
439510307 worker_seed
439510309 worker_seed
Generated
Traceback (most recent call last):
  File "/home/twkim/project/rich_context/textual_inversion/ti_train.py", line 999, in <module>
    main()
  File "/home/twkim/project/rich_context/textual_inversion/ti_train.py", line 830, in main
    caption_log_file=open(caption_log_path,'a')
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/teapot/ti_cnetv4_prior_mlm0001_teapot_mprob015_mbatch25_tagged_mtarget_masked/src/log_captions.txt'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 3.403 MB of 3.403 MB uploaded (0.000 MB deduped)wandb: \ 3.403 MB of 3.403 MB uploaded (0.000 MB deduped)wandb: | 3.403 MB of 3.403 MB uploaded (0.000 MB deduped)wandb: / 3.403 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: - 3.403 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: \ 3.403 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: | 3.403 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: / 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: - 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: \ 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: | 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: / 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb: - 3.429 MB of 3.429 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        loss â–ƒâ–ˆâ–‚â–â–â–â–â–ƒâ–„â–ƒâ–â–…â–‚â–‚â–â–â–â–„â–â–â–‚â–„â–‚â–‚â–ƒâ–â–â–â–ƒâ–‚
wandb:    loss_mlm â–‡â–…â–‡â–‚â–‡â–†â–„â–†â–‚â–†â–ƒâ–ˆâ–‡â–…â–…â–„â–„â–†â–…â–â–„â–…â–ƒâ–â–†â–‡â–†â–…â–…â–ƒ
wandb:          lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   norm_mask â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: norm_target â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:        loss 0.07886
wandb:    loss_mlm 0.05411
wandb:          lr 0.002
wandb:   norm_mask 8.02037
wandb: norm_target 9.50257
wandb: 
wandb: Synced dulcet-salad-574: https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE/runs/51km5vqm
wandb: Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240911_050432-51km5vqm/logs
Traceback (most recent call last):
  File "/home/twkim/anaconda3/envs/context/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/twkim/anaconda3/envs/context/bin/python', 'ti_train.py', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--train_data_dir1=/data/twkim/diffusion/personalization/collected/images/teapot', '--learnable_property=object', '--placeholder_token1=<teapot>', '--train_prior_concept1=teapot', '--eval_prior_concept1=teapot', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--max_train_steps=3001', '--learning_rate=5e-4', '--lr_scheduler=constant', '--initializer_token=teapot', '--normalize_mask_embeds=0', '--lr_warmup_steps=0', '--output_dir=saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/teapot', '--seed=2940', '--mask_tokens=[MASK]', '--lambda_mlm=0.001', '--freeze_mask_embedding=1', '--cls_net_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/cls_net_100000_ckpt.pt', '--mask_embed_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/mask_embeds_100000_ckpt.pt', '--mask_prob=0.15', '--mlm_batch_size=25', '--scale_lr', '--eval_prompt_type=nonliving', '--train_prompt_type=nonliving', '--silent=0', '--rev=0', '--check_tag=VERB-ADJ-ADV-PROPN-ADP-NOUN', '--mlm_target=masked', '--normalize_target1=0', '--caption_root=../datasets_pkgs/captions/v7', '--run_name=ti_cnetv4_prior_mlm0001_teapot_mprob015_mbatch25_tagged_mtarget_masked', '--report_to=wandb', '--project_name=TI MLM SINGLE', '--include_prior_concept=1']' returned non-zero exit status 1.
