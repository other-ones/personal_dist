INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+cu118)
    Python  3.9.18 (you have 3.9.19)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
wandb: Currently logged in as: tuk101234 (qlab-taewook). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/twkim/project/rich_context/textual_inversion/wandb/run-20240911_061910-6jyabeze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-blaze-592
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE
wandb: üöÄ View run at https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE/runs/6jyabeze
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
INFO:__main__:Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'prediction_type', 'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'timestep_spacing', 'thresholding', 'variance_type'} was not found in config. Values will be initialized to default values.
{'scaling_factor'} was not found in config. Values will be initialized to default values.
{'addition_time_embed_dim', 'cross_attention_norm', 'conv_out_kernel', 'resnet_time_scale_shift', 'encoder_hid_dim', 'resnet_out_scale_factor', 'class_embeddings_concat', 'time_embedding_act_fn', 'resnet_skip_time_act', 'mid_block_type', 'transformer_layers_per_block', 'mid_block_only_cross_attention', 'upcast_attention', 'addition_embed_type', 'time_cond_proj_dim', 'time_embedding_type', 'time_embedding_dim', 'addition_embed_type_num_heads', 'conv_in_kernel', 'dual_cross_attention', 'num_class_embeds', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'class_embed_type', 'timestep_post_act', 'num_attention_heads', 'use_linear_projection', 'encoder_hid_dim_type'} was not found in config. Values will be initialized to default values.
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 10000000
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4
INFO:__main__:  Gradient Accumulation steps = 4
INFO:__main__:  Total optimization steps = 3001
set seed 2940
saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/dog6/ti_cnetv4_prior_mlm00005_dog6_mprob015_mbatch25_mtarget_masked_tagged/src/command.txt command_path
ti_train.py item
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 item
--train_data_dir1=/data/twkim/diffusion/personalization/collected/images/dog6 item
--learnable_property=object item
--placeholder_token1=<dog6> item
--train_prior_concept1=dog item
--eval_prior_concept1=dog item
--resolution=512 item
--train_batch_size=1 item
--gradient_accumulation_steps=4 item
--max_train_steps=3001 item
--learning_rate=5e-4 item
--lr_scheduler=constant item
--initializer_token=dog item
--normalize_mask_embeds=0 item
--lr_warmup_steps=0 item
--output_dir=saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/dog6 item
--seed=2940 item
--mask_tokens=[MASK] item
--lambda_mlm=0.0005 item
--freeze_mask_embedding=1 item
--cls_net_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/cls_net_100000_ckpt.pt item
--mask_embed_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/mask_embeds_100000_ckpt.pt item
--mask_prob=0.15 item
--mlm_batch_size=25 item
--scale_lr item
--eval_prompt_type=living item
--train_prompt_type=pet item
--silent=0 item
--rev=0 item
--check_tag=VERB-ADJ-ADV-PROPN-ADP-NOUN item
--mlm_target=masked item
--normalize_target1=0 item
--caption_root=../datasets_pkgs/captions/v7 item
--run_name=ti_cnetv4_prior_mlm00005_dog6_mprob015_mbatch25_mtarget_masked_tagged item
--report_to=wandb item
--project_name=TI MLM SINGLE item
--include_prior_concept=1 item
32 norm_num_groups
320 block_out_channels[0]
32 norm_num_groups
text_model.embeddings.token_embedding.weight text_encoder requires
[49409] mask_token_ids
seeded
captions_pet_backgrounds	552
captions_pet_human_interactions	3528
captions_pet_relations	29376
captions_pet_styles	378
captions_pet_wearings	4560
seeded
captions_pet_backgrounds	552
captions_pet_human_interactions	3528
captions_pet_relations	29376
captions_pet_styles	378
captions_pet_wearings	4560
position_ids defined_key-clsnet
final.weight defined_key-clsnet
final.bias defined_key-clsnet
position_embedding.weight defined_key-clsnet
multi_head_attention1.in_proj_weight defined_key-clsnet
multi_head_attention1.in_proj_bias defined_key-clsnet
multi_head_attention1.out_proj.weight defined_key-clsnet
multi_head_attention1.out_proj.bias defined_key-clsnet
feed_forward1.W_ff1.weight defined_key-clsnet
feed_forward1.W_ff1.bias defined_key-clsnet
feed_forward1.W_ff2.weight defined_key-clsnet
feed_forward1.W_ff2.bias defined_key-clsnet
layer_norm1.weight defined_key-clsnet
layer_norm1.bias defined_key-clsnet
multi_head_attention2.in_proj_weight defined_key-clsnet
multi_head_attention2.in_proj_bias defined_key-clsnet
multi_head_attention2.out_proj.weight defined_key-clsnet
multi_head_attention2.out_proj.bias defined_key-clsnet
layer_norm2.weight defined_key-clsnet
layer_norm2.bias defined_key-clsnet

module.position_ids saved_key-clsnet
module.final.weight saved_key-clsnet
module.final.bias saved_key-clsnet
module.position_embedding.weight saved_key-clsnet
module.multi_head_attention1.in_proj_weight saved_key-clsnet
module.multi_head_attention1.in_proj_bias saved_key-clsnet
module.multi_head_attention1.out_proj.weight saved_key-clsnet
module.multi_head_attention1.out_proj.bias saved_key-clsnet
module.feed_forward1.W_ff1.weight saved_key-clsnet
module.feed_forward1.W_ff1.bias saved_key-clsnet
module.feed_forward1.W_ff2.weight saved_key-clsnet
module.feed_forward1.W_ff2.bias saved_key-clsnet
module.layer_norm1.weight saved_key-clsnet
module.layer_norm1.bias saved_key-clsnet
module.multi_head_attention2.in_proj_weight saved_key-clsnet
module.multi_head_attention2.in_proj_bias saved_key-clsnet
module.multi_head_attention2.out_proj.weight saved_key-clsnet
module.multi_head_attention2.out_proj.bias saved_key-clsnet
module.layer_norm2.weight saved_key-clsnet
module.layer_norm2.bias saved_key-clsnet
Steps:   0% 0/3001 [00:00<?, ?it/s]True accepts_keep_fp32_wrapper
{'keep_fp32_wrapper': True} extra_args
INFO:__main__:STEP 0 Running validation... 
 Generating 7 images with prompt: ['a <dog6> dog in the jungle', 'a <dog6> dog with a city in the background', 'a <dog6> dog with a mountain in the background', 'a <dog6> dog on top of a purple rug in a forest', 'a <dog6> dog in a chef outfit', 'a <dog6> dog in a police outfit', 'a cube shaped <dog6> dog'].

  0% 0/25 [00:00<?, ?it/s][A

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Step		|0
Raw		|a          close      -          up         photo      of         the        <dog6>     dog        above      a          M[teal]    pillow    
Masked		|a          close      -          up         photo      of         the        <dog6>     dog        above      a          [MASK]     pillow    
Preds		|a          close      -          up         photo      of         the        dog        dog        above      a          M[large]   pillow    
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------


  4% 1/25 [00:00<00:08,  2.77it/s][A
  8% 2/25 [00:00<00:07,  3.00it/s][A
 12% 3/25 [00:00<00:07,  3.08it/s][A
 16% 4/25 [00:01<00:06,  3.12it/s][A
 20% 5/25 [00:01<00:06,  3.15it/s][A
 24% 6/25 [00:01<00:05,  3.17it/s][A
 28% 7/25 [00:02<00:05,  3.17it/s][A
 32% 8/25 [00:02<00:05,  3.17it/s][A
 36% 9/25 [00:02<00:05,  3.18it/s][A
 40% 10/25 [00:03<00:04,  3.18it/s][A
 44% 11/25 [00:03<00:04,  3.17it/s][A
 48% 12/25 [00:03<00:04,  3.18it/s][A
 52% 13/25 [00:04<00:03,  3.18it/s][A
 56% 14/25 [00:04<00:03,  3.18it/s][A
 60% 15/25 [00:04<00:03,  3.17it/s][A
 64% 16/25 [00:05<00:02,  3.18it/s][A
 68% 17/25 [00:05<00:02,  3.18it/s][A
 72% 18/25 [00:05<00:02,  3.18it/s][A
 76% 19/25 [00:06<00:01,  3.18it/s][A
 80% 20/25 [00:06<00:01,  3.18it/s][A
 84% 21/25 [00:06<00:01,  3.18it/s][A
 88% 22/25 [00:06<00:00,  3.18it/s][A
 92% 23/25 [00:07<00:00,  3.17it/s][A
 96% 24/25 [00:07<00:00,  3.18it/s][A
100% 25/25 [00:07<00:00,  3.17it/s][A100% 25/25 [00:08<00:00,  2.84it/s]
Steps:   0% 0/3001 [00:19<?, ?it/s, _runtime=31, _timestamp=1.73e+9, loss=0.122, loss_mlm=0.0626, lr=0.002, norm_mask=8.02, norm_target=8.12]Steps:   0% 1/3001 [00:19<16:38:47, 19.98s/it, _runtime=31, _timestamp=1.73e+9, loss=0.122, loss_mlm=0.0626, lr=0.002, norm_mask=8.02, norm_target=8.12]Steps:   0% 1/3001 [00:21<16:38:47, 19.98s/it, _runtime=32, _timestamp=1.73e+9, loss=0.516, loss_mlm=0.1, lr=0.002, norm_mask=8.02, norm_target=8.12]   Steps:   0% 2/3001 [00:21<7:29:04,  8.98s/it, _runtime=32, _timestamp=1.73e+9, loss=0.516, loss_mlm=0.1, lr=0.002, norm_mask=8.02, norm_target=8.12] Steps:   0% 2/3001 [00:22<7:29:04,  8.98s/it, _runtime=33, _timestamp=1.73e+9, loss=0.0634, loss_mlm=0.0688, lr=0.002, norm_mask=8.02, norm_target=8.12]Steps:   0% 3/3001 [00:22<4:31:54,  5.44s/it, _runtime=33, _timestamp=1.73e+9, loss=0.0634, loss_mlm=0.0688, lr=0.002, norm_mask=8.02, norm_target=8.12]Steps:   0% 3/3001 [00:23<4:31:54,  5.44s/it, _runtime=35, _timestamp=1.73e+9, loss=0.00295, loss_mlm=0.063, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 4/3001 [00:23<3:08:47,  3.78s/it, _runtime=35, _timestamp=1.73e+9, loss=0.00295, loss_mlm=0.063, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 4/3001 [00:24<3:08:47,  3.78s/it, _runtime=36, _timestamp=1.73e+9, loss=0.00295, loss_mlm=0.0995, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 5/3001 [00:24<2:22:36,  2.86s/it, _runtime=36, _timestamp=1.73e+9, loss=0.00295, loss_mlm=0.0995, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 5/3001 [00:26<2:22:36,  2.86s/it, _runtime=37, _timestamp=1.73e+9, loss=0.00535, loss_mlm=0.0864, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 6/3001 [00:26<1:54:35,  2.30s/it, _runtime=37, _timestamp=1.73e+9, loss=0.00535, loss_mlm=0.0864, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 6/3001 [00:27<1:54:35,  2.30s/it, _runtime=38, _timestamp=1.73e+9, loss=0.0236, loss_mlm=0.104, lr=0.002, norm_mask=8.02, norm_target=8.16]  Steps:   0% 7/3001 [00:27<1:36:58,  1.94s/it, _runtime=38, _timestamp=1.73e+9, loss=0.0236, loss_mlm=0.104, lr=0.002, norm_mask=8.02, norm_target=8.16]Steps:   0% 7/3001 [00:29<1:36:58,  1.94s/it, _runtime=41, _timestamp=1.73e+9, loss=0.122, loss_mlm=0.0535, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 8/3001 [00:29<1:41:04,  2.03s/it, _runtime=41, _timestamp=1.73e+9, loss=0.122, loss_mlm=0.0535, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 8/3001 [00:30<1:41:04,  2.03s/it, _runtime=42, _timestamp=1.73e+9, loss=0.141, loss_mlm=0.0675, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 9/3001 [00:30<1:29:54,  1.80s/it, _runtime=42, _timestamp=1.73e+9, loss=0.141, loss_mlm=0.0675, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 9/3001 [00:32<1:29:54,  1.80s/it, _runtime=43, _timestamp=1.73e+9, loss=0.12, loss_mlm=0.0759, lr=0.002, norm_mask=8.02, norm_target=8.24] Steps:   0% 10/3001 [00:32<1:20:40,  1.62s/it, _runtime=43, _timestamp=1.73e+9, loss=0.12, loss_mlm=0.0759, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 10/3001 [00:33<1:20:40,  1.62s/it, _runtime=44, _timestamp=1.73e+9, loss=0.0164, loss_mlm=0.0376, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 11/3001 [00:33<1:14:36,  1.50s/it, _runtime=44, _timestamp=1.73e+9, loss=0.0164, loss_mlm=0.0376, lr=0.002, norm_mask=8.02, norm_target=8.24]Steps:   0% 11/3001 [00:34<1:14:36,  1.50s/it, _runtime=45, _timestamp=1.73e+9, loss=0.175, loss_mlm=0.103, lr=0.002, norm_mask=8.02, norm_target=8.33]  Steps:   0% 12/3001 [00:34<1:10:07,  1.41s/it, _runtime=45, _timestamp=1.73e+9, loss=0.175, loss_mlm=0.103, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 12/3001 [00:35<1:10:07,  1.41s/it, _runtime=47, _timestamp=1.73e+9, loss=0.0847, loss_mlm=0.137, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 13/3001 [00:35<1:07:01,  1.35s/it, _runtime=47, _timestamp=1.73e+9, loss=0.0847, loss_mlm=0.137, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 13/3001 [00:36<1:07:01,  1.35s/it, _runtime=48, _timestamp=1.73e+9, loss=0.0449, loss_mlm=0.0777, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 14/3001 [00:36<1:04:49,  1.30s/it, _runtime=48, _timestamp=1.73e+9, loss=0.0449, loss_mlm=0.0777, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 14/3001 [00:38<1:04:49,  1.30s/it, _runtime=49, _timestamp=1.73e+9, loss=0.0118, loss_mlm=0.111, lr=0.002, norm_mask=8.02, norm_target=8.33] Steps:   0% 15/3001 [00:38<1:03:34,  1.28s/it, _runtime=49, _timestamp=1.73e+9, loss=0.0118, loss_mlm=0.111, lr=0.002, norm_mask=8.02, norm_target=8.33]Steps:   0% 15/3001 [00:39<1:03:34,  1.28s/it, _runtime=50, _timestamp=1.73e+9, loss=0.00862, loss_mlm=0.0918, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 16/3001 [00:39<1:02:30,  1.26s/it, _runtime=50, _timestamp=1.73e+9, loss=0.00862, loss_mlm=0.0918, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 16/3001 [00:41<1:02:30,  1.26s/it, _runtime=52, _timestamp=1.73e+9, loss=0.0116, loss_mlm=0.0492, lr=0.002, norm_mask=8.02, norm_target=8.41] Steps:   1% 17/3001 [00:41<1:09:16,  1.39s/it, _runtime=52, _timestamp=1.73e+9, loss=0.0116, loss_mlm=0.0492, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 17/3001 [00:42<1:09:16,  1.39s/it, _runtime=54, _timestamp=1.73e+9, loss=0.23, loss_mlm=0.0488, lr=0.002, norm_mask=8.02, norm_target=8.41]  Steps:   1% 18/3001 [00:42<1:15:49,  1.53s/it, _runtime=54, _timestamp=1.73e+9, loss=0.23, loss_mlm=0.0488, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 18/3001 [00:44<1:15:49,  1.53s/it, _runtime=55, _timestamp=1.73e+9, loss=0.0111, loss_mlm=0.0605, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 19/3001 [00:44<1:10:57,  1.43s/it, _runtime=55, _timestamp=1.73e+9, loss=0.0111, loss_mlm=0.0605, lr=0.002, norm_mask=8.02, norm_target=8.41]Steps:   1% 19/3001 [00:45<1:10:57,  1.43s/it, _runtime=57, _timestamp=1.73e+9, loss=0.00182, loss_mlm=0.0578, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 20/3001 [00:45<1:13:09,  1.47s/it, _runtime=57, _timestamp=1.73e+9, loss=0.00182, loss_mlm=0.0578, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 20/3001 [00:46<1:13:09,  1.47s/it, _runtime=58, _timestamp=1.73e+9, loss=0.0741, loss_mlm=0.0614, lr=0.002, norm_mask=8.02, norm_target=8.54] Steps:   1% 21/3001 [00:46<1:09:18,  1.40s/it, _runtime=58, _timestamp=1.73e+9, loss=0.0741, loss_mlm=0.0614, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 21/3001 [00:48<1:09:18,  1.40s/it, _runtime=59, _timestamp=1.73e+9, loss=0.162, loss_mlm=0.0731, lr=0.002, norm_mask=8.02, norm_target=8.54] Steps:   1% 22/3001 [00:48<1:06:35,  1.34s/it, _runtime=59, _timestamp=1.73e+9, loss=0.162, loss_mlm=0.0731, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 22/3001 [00:49<1:06:35,  1.34s/it, _runtime=60, _timestamp=1.73e+9, loss=0.0399, loss_mlm=0.0877, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 23/3001 [00:49<1:04:30,  1.30s/it, _runtime=60, _timestamp=1.73e+9, loss=0.0399, loss_mlm=0.0877, lr=0.002, norm_mask=8.02, norm_target=8.54]Steps:   1% 23/3001 [00:50<1:04:30,  1.30s/it, _runtime=61, _timestamp=1.73e+9, loss=0.0803, loss_mlm=0.0773, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 24/3001 [00:50<1:03:13,  1.27s/it, _runtime=61, _timestamp=1.73e+9, loss=0.0803, loss_mlm=0.0773, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 24/3001 [00:51<1:03:13,  1.27s/it, _runtime=63, _timestamp=1.73e+9, loss=0.0751, loss_mlm=0.11, lr=0.002, norm_mask=8.02, norm_target=8.68]  Steps:   1% 25/3001 [00:51<1:02:38,  1.26s/it, _runtime=63, _timestamp=1.73e+9, loss=0.0751, loss_mlm=0.11, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 25/3001 [00:53<1:02:38,  1.26s/it, _runtime=64, _timestamp=1.73e+9, loss=0.0101, loss_mlm=0.0818, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 26/3001 [00:53<1:09:09,  1.39s/it, _runtime=64, _timestamp=1.73e+9, loss=0.0101, loss_mlm=0.0818, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 26/3001 [00:55<1:09:09,  1.39s/it, _runtime=66, _timestamp=1.73e+9, loss=0.0128, loss_mlm=0.0905, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 27/3001 [00:55<1:15:47,  1.53s/it, _runtime=66, _timestamp=1.73e+9, loss=0.0128, loss_mlm=0.0905, lr=0.002, norm_mask=8.02, norm_target=8.68]Steps:   1% 27/3001 [00:56<1:15:47,  1.53s/it, _runtime=67, _timestamp=1.73e+9, loss=0.0114, loss_mlm=0.0447, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 28/3001 [00:56<1:11:01,  1.43s/it, _runtime=67, _timestamp=1.73e+9, loss=0.0114, loss_mlm=0.0447, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 28/3001 [00:57<1:11:01,  1.43s/it, _runtime=69, _timestamp=1.73e+9, loss=0.173, loss_mlm=0.0546, lr=0.002, norm_mask=8.02, norm_target=8.82] Steps:   1% 29/3001 [00:57<1:07:35,  1.36s/it, _runtime=69, _timestamp=1.73e+9, loss=0.173, loss_mlm=0.0546, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 29/3001 [00:58<1:07:35,  1.36s/it, _runtime=70, _timestamp=1.73e+9, loss=0.0803, loss_mlm=0.0623, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 30/3001 [00:58<1:05:16,  1.32s/it, _runtime=70, _timestamp=1.73e+9, loss=0.0803, loss_mlm=0.0623, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 30/3001 [01:00<1:05:16,  1.32s/it, _runtime=71, _timestamp=1.73e+9, loss=0.196, loss_mlm=0.0857, lr=0.002, norm_mask=8.02, norm_target=8.82] Steps:   1% 31/3001 [01:00<1:03:30,  1.28s/it, _runtime=71, _timestamp=1.73e+9, loss=0.196, loss_mlm=0.0857, lr=0.002, norm_mask=8.02, norm_target=8.82]Steps:   1% 31/3001 [01:01<1:03:30,  1.28s/it, _runtime=72, _timestamp=1.73e+9, loss=0.105, loss_mlm=0.0793, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 32/3001 [01:01<1:02:18,  1.26s/it, _runtime=72, _timestamp=1.73e+9, loss=0.105, loss_mlm=0.0793, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 32/3001 [01:02<1:02:18,  1.26s/it, _runtime=73, _timestamp=1.73e+9, loss=0.141, loss_mlm=0.0537, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 33/3001 [01:02<1:01:21,  1.24s/it, _runtime=73, _timestamp=1.73e+9, loss=0.141, loss_mlm=0.0537, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 33/3001 [01:03<1:01:21,  1.24s/it, _runtime=75, _timestamp=1.73e+9, loss=0.0399, loss_mlm=0.0881, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 34/3001 [01:03<1:00:51,  1.23s/it, _runtime=75, _timestamp=1.73e+9, loss=0.0399, loss_mlm=0.0881, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 34/3001 [01:04<1:00:51,  1.23s/it, _runtime=76, _timestamp=1.73e+9, loss=0.0243, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=8.98] Steps:   1% 35/3001 [01:04<1:00:26,  1.22s/it, _runtime=76, _timestamp=1.73e+9, loss=0.0243, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=8.98]Steps:   1% 35/3001 [01:07<1:00:26,  1.22s/it, _runtime=78, _timestamp=1.73e+9, loss=0.0559, loss_mlm=0.0853, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 36/3001 [01:07<1:17:35,  1.57s/it, _runtime=78, _timestamp=1.73e+9, loss=0.0559, loss_mlm=0.0853, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 36/3001 [01:08<1:17:35,  1.57s/it, _runtime=79, _timestamp=1.73e+9, loss=0.0075, loss_mlm=0.0533, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 37/3001 [01:08<1:12:16,  1.46s/it, _runtime=79, _timestamp=1.73e+9, loss=0.0075, loss_mlm=0.0533, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 37/3001 [01:09<1:12:16,  1.46s/it, _runtime=81, _timestamp=1.73e+9, loss=0.0287, loss_mlm=0.0763, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 38/3001 [01:09<1:08:22,  1.38s/it, _runtime=81, _timestamp=1.73e+9, loss=0.0287, loss_mlm=0.0763, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 38/3001 [01:10<1:08:22,  1.38s/it, _runtime=82, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.0814, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 39/3001 [01:10<1:05:54,  1.34s/it, _runtime=82, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.0814, lr=0.002, norm_mask=8.02, norm_target=9.13]Steps:   1% 39/3001 [01:12<1:05:54,  1.34s/it, _runtime=83, _timestamp=1.73e+9, loss=0.0225, loss_mlm=0.0841, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 40/3001 [01:12<1:03:55,  1.30s/it, _runtime=83, _timestamp=1.73e+9, loss=0.0225, loss_mlm=0.0841, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 40/3001 [01:13<1:03:55,  1.30s/it, _runtime=84, _timestamp=1.73e+9, loss=0.0041, loss_mlm=0.1, lr=0.002, norm_mask=8.02, norm_target=9.25]   Steps:   1% 41/3001 [01:13<1:02:43,  1.27s/it, _runtime=84, _timestamp=1.73e+9, loss=0.0041, loss_mlm=0.1, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 41/3001 [01:14<1:02:43,  1.27s/it, _runtime=86, _timestamp=1.73e+9, loss=0.0422, loss_mlm=0.0517, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 42/3001 [01:14<1:01:38,  1.25s/it, _runtime=86, _timestamp=1.73e+9, loss=0.0422, loss_mlm=0.0517, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 42/3001 [01:15<1:01:38,  1.25s/it, _runtime=87, _timestamp=1.73e+9, loss=0.239, loss_mlm=0.0839, lr=0.002, norm_mask=8.02, norm_target=9.25] Steps:   1% 43/3001 [01:15<1:01:09,  1.24s/it, _runtime=87, _timestamp=1.73e+9, loss=0.239, loss_mlm=0.0839, lr=0.002, norm_mask=8.02, norm_target=9.25]Steps:   1% 43/3001 [01:17<1:01:09,  1.24s/it, _runtime=88, _timestamp=1.73e+9, loss=0.00372, loss_mlm=0.0835, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   1% 44/3001 [01:17<1:05:30,  1.33s/it, _runtime=88, _timestamp=1.73e+9, loss=0.00372, loss_mlm=0.0835, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   1% 44/3001 [01:19<1:05:30,  1.33s/it, _runtime=91, _timestamp=1.73e+9, loss=0.00894, loss_mlm=0.0424, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   1% 45/3001 [01:19<1:20:57,  1.64s/it, _runtime=91, _timestamp=1.73e+9, loss=0.00894, loss_mlm=0.0424, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   1% 45/3001 [01:20<1:20:57,  1.64s/it, _runtime=92, _timestamp=1.73e+9, loss=0.00296, loss_mlm=0.0565, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   2% 46/3001 [01:20<1:15:05,  1.52s/it, _runtime=92, _timestamp=1.73e+9, loss=0.00296, loss_mlm=0.0565, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   2% 46/3001 [01:22<1:15:05,  1.52s/it, _runtime=93, _timestamp=1.73e+9, loss=0.00531, loss_mlm=0.0807, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   2% 47/3001 [01:22<1:10:18,  1.43s/it, _runtime=93, _timestamp=1.73e+9, loss=0.00531, loss_mlm=0.0807, lr=0.002, norm_mask=8.02, norm_target=9.38]Steps:   2% 47/3001 [01:23<1:10:18,  1.43s/it, _runtime=94, _timestamp=1.73e+9, loss=0.258, loss_mlm=0.0564, lr=0.002, norm_mask=8.02, norm_target=9.51]  Steps:   2% 48/3001 [01:23<1:07:04,  1.36s/it, _runtime=94, _timestamp=1.73e+9, loss=0.258, loss_mlm=0.0564, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 48/3001 [01:24<1:07:04,  1.36s/it, _runtime=96, _timestamp=1.73e+9, loss=0.00185, loss_mlm=0.0504, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 49/3001 [01:24<1:04:44,  1.32s/it, _runtime=96, _timestamp=1.73e+9, loss=0.00185, loss_mlm=0.0504, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 49/3001 [01:25<1:04:44,  1.32s/it, _runtime=97, _timestamp=1.73e+9, loss=0.00253, loss_mlm=0.0687, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 50/3001 [01:25<1:03:19,  1.29s/it, _runtime=97, _timestamp=1.73e+9, loss=0.00253, loss_mlm=0.0687, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 50/3001 [01:27<1:03:19,  1.29s/it, _runtime=98, _timestamp=1.73e+9, loss=0.00596, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 51/3001 [01:27<1:02:19,  1.27s/it, _runtime=98, _timestamp=1.73e+9, loss=0.00596, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=9.51]Steps:   2% 51/3001 [01:28<1:02:19,  1.27s/it, _runtime=99, _timestamp=1.73e+9, loss=0.43, loss_mlm=0.0845, lr=0.002, norm_mask=8.02, norm_target=9.65]   Steps:   2% 52/3001 [01:28<1:01:17,  1.25s/it, _runtime=99, _timestamp=1.73e+9, loss=0.43, loss_mlm=0.0845, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 52/3001 [01:29<1:01:17,  1.25s/it, _runtime=100, _timestamp=1.73e+9, loss=0.00195, loss_mlm=0.0326, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 53/3001 [01:29<1:00:32,  1.23s/it, _runtime=100, _timestamp=1.73e+9, loss=0.00195, loss_mlm=0.0326, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 53/3001 [01:31<1:00:32,  1.23s/it, _runtime=103, _timestamp=1.73e+9, loss=0.00277, loss_mlm=0.0673, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 54/3001 [01:31<1:17:00,  1.57s/it, _runtime=103, _timestamp=1.73e+9, loss=0.00277, loss_mlm=0.0673, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 54/3001 [01:33<1:17:00,  1.57s/it, _runtime=104, _timestamp=1.73e+9, loss=0.0492, loss_mlm=0.0443, lr=0.002, norm_mask=8.02, norm_target=9.65] Steps:   2% 55/3001 [01:33<1:12:23,  1.47s/it, _runtime=104, _timestamp=1.73e+9, loss=0.0492, loss_mlm=0.0443, lr=0.002, norm_mask=8.02, norm_target=9.65]Steps:   2% 55/3001 [01:34<1:12:23,  1.47s/it, _runtime=105, _timestamp=1.73e+9, loss=0.316, loss_mlm=0.0995, lr=0.002, norm_mask=8.02, norm_target=9.79] Steps:   2% 56/3001 [01:34<1:08:43,  1.40s/it, _runtime=105, _timestamp=1.73e+9, loss=0.316, loss_mlm=0.0995, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 56/3001 [01:35<1:08:43,  1.40s/it, _runtime=106, _timestamp=1.73e+9, loss=0.274, loss_mlm=0.0706, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 57/3001 [01:35<1:05:49,  1.34s/it, _runtime=106, _timestamp=1.73e+9, loss=0.274, loss_mlm=0.0706, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 57/3001 [01:36<1:05:49,  1.34s/it, _runtime=108, _timestamp=1.73e+9, loss=0.103, loss_mlm=0.0522, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 58/3001 [01:36<1:04:18,  1.31s/it, _runtime=108, _timestamp=1.73e+9, loss=0.103, loss_mlm=0.0522, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 58/3001 [01:37<1:04:18,  1.31s/it, _runtime=109, _timestamp=1.73e+9, loss=0.0424, loss_mlm=0.083, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 59/3001 [01:37<1:03:22,  1.29s/it, _runtime=109, _timestamp=1.73e+9, loss=0.0424, loss_mlm=0.083, lr=0.002, norm_mask=8.02, norm_target=9.79]Steps:   2% 59/3001 [01:39<1:03:22,  1.29s/it, _runtime=110, _timestamp=1.73e+9, loss=0.0113, loss_mlm=0.0944, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 60/3001 [01:39<1:02:19,  1.27s/it, _runtime=110, _timestamp=1.73e+9, loss=0.0113, loss_mlm=0.0944, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 60/3001 [01:40<1:02:19,  1.27s/it, _runtime=111, _timestamp=1.73e+9, loss=0.0124, loss_mlm=0.0317, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 61/3001 [01:40<1:01:39,  1.26s/it, _runtime=111, _timestamp=1.73e+9, loss=0.0124, loss_mlm=0.0317, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 61/3001 [01:41<1:01:39,  1.26s/it, _runtime=113, _timestamp=1.73e+9, loss=0.158, loss_mlm=0.074, lr=0.002, norm_mask=8.02, norm_target=9.91]  Steps:   2% 62/3001 [01:41<1:01:14,  1.25s/it, _runtime=113, _timestamp=1.73e+9, loss=0.158, loss_mlm=0.074, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 62/3001 [01:43<1:01:14,  1.25s/it, _runtime=115, _timestamp=1.73e+9, loss=0.0309, loss_mlm=0.108, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 63/3001 [01:43<1:16:09,  1.56s/it, _runtime=115, _timestamp=1.73e+9, loss=0.0309, loss_mlm=0.108, lr=0.002, norm_mask=8.02, norm_target=9.91]Steps:   2% 63/3001 [01:45<1:16:09,  1.56s/it, _runtime=116, _timestamp=1.73e+9, loss=0.0722, loss_mlm=0.0346, lr=0.002, norm_mask=8.02, norm_target=10] Steps:   2% 64/3001 [01:45<1:12:00,  1.47s/it, _runtime=116, _timestamp=1.73e+9, loss=0.0722, loss_mlm=0.0346, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 64/3001 [01:46<1:12:00,  1.47s/it, _runtime=117, _timestamp=1.73e+9, loss=0.0324, loss_mlm=0.0459, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 65/3001 [01:46<1:08:21,  1.40s/it, _runtime=117, _timestamp=1.73e+9, loss=0.0324, loss_mlm=0.0459, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 65/3001 [01:47<1:08:21,  1.40s/it, _runtime=119, _timestamp=1.73e+9, loss=0.00531, loss_mlm=0.0685, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 66/3001 [01:47<1:06:12,  1.35s/it, _runtime=119, _timestamp=1.73e+9, loss=0.00531, loss_mlm=0.0685, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 66/3001 [01:48<1:06:12,  1.35s/it, _runtime=120, _timestamp=1.73e+9, loss=0.07, loss_mlm=0.0755, lr=0.002, norm_mask=8.02, norm_target=10]   Steps:   2% 67/3001 [01:48<1:04:15,  1.31s/it, _runtime=120, _timestamp=1.73e+9, loss=0.07, loss_mlm=0.0755, lr=0.002, norm_mask=8.02, norm_target=10]Steps:   2% 67/3001 [01:50<1:04:15,  1.31s/it, _runtime=121, _timestamp=1.73e+9, loss=0.01, loss_mlm=0.071, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 68/3001 [01:50<1:07:47,  1.39s/it, _runtime=121, _timestamp=1.73e+9, loss=0.01, loss_mlm=0.071, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 68/3001 [01:51<1:07:47,  1.39s/it, _runtime=123, _timestamp=1.73e+9, loss=0.0201, loss_mlm=0.0886, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 69/3001 [01:51<1:05:29,  1.34s/it, _runtime=123, _timestamp=1.73e+9, loss=0.0201, loss_mlm=0.0886, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 69/3001 [01:52<1:05:29,  1.34s/it, _runtime=124, _timestamp=1.73e+9, loss=0.0734, loss_mlm=0.104, lr=0.002, norm_mask=8.02, norm_target=10.1] Steps:   2% 70/3001 [01:52<1:03:50,  1.31s/it, _runtime=124, _timestamp=1.73e+9, loss=0.0734, loss_mlm=0.104, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 70/3001 [01:54<1:03:50,  1.31s/it, _runtime=125, _timestamp=1.73e+9, loss=0.0558, loss_mlm=0.0925, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 71/3001 [01:54<1:02:45,  1.29s/it, _runtime=125, _timestamp=1.73e+9, loss=0.0558, loss_mlm=0.0925, lr=0.002, norm_mask=8.02, norm_target=10.1]Steps:   2% 71/3001 [01:56<1:02:45,  1.29s/it, _runtime=127, _timestamp=1.73e+9, loss=0.101, loss_mlm=0.0828, lr=0.002, norm_mask=8.02, norm_target=10.3] Steps:   2% 72/3001 [01:56<1:17:15,  1.58s/it, _runtime=127, _timestamp=1.73e+9, loss=0.101, loss_mlm=0.0828, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 72/3001 [01:57<1:17:15,  1.58s/it, _runtime=129, _timestamp=1.73e+9, loss=0.746, loss_mlm=0.107, lr=0.002, norm_mask=8.02, norm_target=10.3] Steps:   2% 73/3001 [01:57<1:12:13,  1.48s/it, _runtime=129, _timestamp=1.73e+9, loss=0.746, loss_mlm=0.107, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 73/3001 [01:58<1:12:13,  1.48s/it, _runtime=130, _timestamp=1.73e+9, loss=0.175, loss_mlm=0.0727, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 74/3001 [01:58<1:08:31,  1.40s/it, _runtime=130, _timestamp=1.73e+9, loss=0.175, loss_mlm=0.0727, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 74/3001 [02:00<1:08:31,  1.40s/it, _runtime=131, _timestamp=1.73e+9, loss=0.0992, loss_mlm=0.0717, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 75/3001 [02:00<1:05:54,  1.35s/it, _runtime=131, _timestamp=1.73e+9, loss=0.0992, loss_mlm=0.0717, lr=0.002, norm_mask=8.02, norm_target=10.3]Steps:   2% 75/3001 [02:01<1:05:54,  1.35s/it, _runtime=132, _timestamp=1.73e+9, loss=0.00364, loss_mlm=0.0746, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 76/3001 [02:01<1:03:47,  1.31s/it, _runtime=132, _timestamp=1.73e+9, loss=0.00364, loss_mlm=0.0746, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 76/3001 [02:02<1:03:47,  1.31s/it, _runtime=133, _timestamp=1.73e+9, loss=0.0291, loss_mlm=0.0667, lr=0.002, norm_mask=8.02, norm_target=10.4] Steps:   3% 77/3001 [02:02<1:02:26,  1.28s/it, _runtime=133, _timestamp=1.73e+9, loss=0.0291, loss_mlm=0.0667, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 77/3001 [02:03<1:02:26,  1.28s/it, _runtime=135, _timestamp=1.73e+9, loss=0.00483, loss_mlm=0.0756, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 78/3001 [02:03<1:01:42,  1.27s/it, _runtime=135, _timestamp=1.73e+9, loss=0.00483, loss_mlm=0.0756, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 78/3001 [02:04<1:01:42,  1.27s/it, _runtime=136, _timestamp=1.73e+9, loss=0.0107, loss_mlm=0.0895, lr=0.002, norm_mask=8.02, norm_target=10.4] Steps:   3% 79/3001 [02:04<1:01:05,  1.25s/it, _runtime=136, _timestamp=1.73e+9, loss=0.0107, loss_mlm=0.0895, lr=0.002, norm_mask=8.02, norm_target=10.4]Steps:   3% 79/3001 [02:06<1:01:05,  1.25s/it, _runtime=137, _timestamp=1.73e+9, loss=0.0746, loss_mlm=0.0825, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 80/3001 [02:06<1:00:36,  1.25s/it, _runtime=137, _timestamp=1.73e+9, loss=0.0746, loss_mlm=0.0825, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 80/3001 [02:08<1:00:36,  1.25s/it, _runtime=139, _timestamp=1.73e+9, loss=0.0551, loss_mlm=0.113, lr=0.002, norm_mask=8.02, norm_target=10.5] Steps:   3% 81/3001 [02:08<1:15:07,  1.54s/it, _runtime=139, _timestamp=1.73e+9, loss=0.0551, loss_mlm=0.113, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 81/3001 [02:09<1:15:07,  1.54s/it, _runtime=141, _timestamp=1.73e+9, loss=0.00497, loss_mlm=0.0602, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 82/3001 [02:09<1:11:32,  1.47s/it, _runtime=141, _timestamp=1.73e+9, loss=0.00497, loss_mlm=0.0602, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 82/3001 [02:10<1:11:32,  1.47s/it, _runtime=142, _timestamp=1.73e+9, loss=0.0327, loss_mlm=0.09, lr=0.002, norm_mask=8.02, norm_target=10.5]   Steps:   3% 83/3001 [02:10<1:07:52,  1.40s/it, _runtime=142, _timestamp=1.73e+9, loss=0.0327, loss_mlm=0.09, lr=0.002, norm_mask=8.02, norm_target=10.5]Steps:   3% 83/3001 [02:12<1:07:52,  1.40s/it, _runtime=143, _timestamp=1.73e+9, loss=0.293, loss_mlm=0.0866, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 84/3001 [02:12<1:05:14,  1.34s/it, _runtime=143, _timestamp=1.73e+9, loss=0.293, loss_mlm=0.0866, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 84/3001 [02:13<1:05:14,  1.34s/it, _runtime=144, _timestamp=1.73e+9, loss=0.0551, loss_mlm=0.0837, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 85/3001 [02:13<1:03:24,  1.30s/it, _runtime=144, _timestamp=1.73e+9, loss=0.0551, loss_mlm=0.0837, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 85/3001 [02:14<1:03:24,  1.30s/it, _runtime=146, _timestamp=1.73e+9, loss=0.201, loss_mlm=0.0833, lr=0.002, norm_mask=8.02, norm_target=10.6] Steps:   3% 86/3001 [02:14<1:02:10,  1.28s/it, _runtime=146, _timestamp=1.73e+9, loss=0.201, loss_mlm=0.0833, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 86/3001 [02:15<1:02:10,  1.28s/it, _runtime=147, _timestamp=1.73e+9, loss=0.00727, loss_mlm=0.0648, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 87/3001 [02:15<1:01:21,  1.26s/it, _runtime=147, _timestamp=1.73e+9, loss=0.00727, loss_mlm=0.0648, lr=0.002, norm_mask=8.02, norm_target=10.6]Steps:   3% 87/3001 [02:17<1:01:21,  1.26s/it, _runtime=148, _timestamp=1.73e+9, loss=0.0169, loss_mlm=0.0589, lr=0.002, norm_mask=8.02, norm_target=10.7] Steps:   3% 88/3001 [02:17<1:00:56,  1.26s/it, _runtime=148, _timestamp=1.73e+9, loss=0.0169, loss_mlm=0.0589, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 88/3001 [02:18<1:00:56,  1.26s/it, _runtime=149, _timestamp=1.73e+9, loss=0.00592, loss_mlm=0.0869, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 89/3001 [02:18<1:00:26,  1.25s/it, _runtime=149, _timestamp=1.73e+9, loss=0.00592, loss_mlm=0.0869, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 89/3001 [02:20<1:00:26,  1.25s/it, _runtime=151, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.137, lr=0.002, norm_mask=8.02, norm_target=10.7]  Steps:   3% 90/3001 [02:20<1:13:56,  1.52s/it, _runtime=151, _timestamp=1.73e+9, loss=0.0121, loss_mlm=0.137, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 90/3001 [02:22<1:13:56,  1.52s/it, _runtime=153, _timestamp=1.73e+9, loss=0.0815, loss_mlm=0.136, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 91/3001 [02:22<1:14:30,  1.54s/it, _runtime=153, _timestamp=1.73e+9, loss=0.0815, loss_mlm=0.136, lr=0.002, norm_mask=8.02, norm_target=10.7]Steps:   3% 91/3001 [02:23<1:14:30,  1.54s/it, _runtime=154, _timestamp=1.73e+9, loss=0.0136, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 92/3001 [02:23<1:09:56,  1.44s/it, _runtime=154, _timestamp=1.73e+9, loss=0.0136, loss_mlm=0.102, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 92/3001 [02:24<1:09:56,  1.44s/it, _runtime=155, _timestamp=1.73e+9, loss=0.0229, loss_mlm=0.0814, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 93/3001 [02:24<1:06:37,  1.37s/it, _runtime=155, _timestamp=1.73e+9, loss=0.0229, loss_mlm=0.0814, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 93/3001 [02:25<1:06:37,  1.37s/it, _runtime=157, _timestamp=1.73e+9, loss=0.0069, loss_mlm=0.0965, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 94/3001 [02:25<1:04:11,  1.32s/it, _runtime=157, _timestamp=1.73e+9, loss=0.0069, loss_mlm=0.0965, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 94/3001 [02:26<1:04:11,  1.32s/it, _runtime=158, _timestamp=1.73e+9, loss=0.0167, loss_mlm=0.0553, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 95/3001 [02:26<1:02:39,  1.29s/it, _runtime=158, _timestamp=1.73e+9, loss=0.0167, loss_mlm=0.0553, lr=0.002, norm_mask=8.02, norm_target=10.8]Steps:   3% 95/3001 [02:28<1:02:39,  1.29s/it, _runtime=159, _timestamp=1.73e+9, loss=0.118, loss_mlm=0.052, lr=0.002, norm_mask=8.02, norm_target=10.9]  Steps:   3% 96/3001 [02:28<1:01:30,  1.27s/it, _runtime=159, _timestamp=1.73e+9, loss=0.118, loss_mlm=0.052, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 96/3001 [02:29<1:01:30,  1.27s/it, _runtime=160, _timestamp=1.73e+9, loss=0.14, loss_mlm=0.0753, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 97/3001 [02:29<1:00:39,  1.25s/it, _runtime=160, _timestamp=1.73e+9, loss=0.14, loss_mlm=0.0753, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 97/3001 [02:30<1:00:39,  1.25s/it, _runtime=162, _timestamp=1.73e+9, loss=0.00542, loss_mlm=0.11, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 98/3001 [02:30<1:00:03,  1.24s/it, _runtime=162, _timestamp=1.73e+9, loss=0.00542, loss_mlm=0.11, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 98/3001 [02:32<1:00:03,  1.24s/it, _runtime=164, _timestamp=1.73e+9, loss=0.00643, loss_mlm=0.0533, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 99/3001 [02:32<1:15:46,  1.57s/it, _runtime=164, _timestamp=1.73e+9, loss=0.00643, loss_mlm=0.0533, lr=0.002, norm_mask=8.02, norm_target=10.9]Steps:   3% 99/3001 [02:34<1:15:46,  1.57s/it, _runtime=165, _timestamp=1.73e+9, loss=0.054, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=11.1]  Steps:   3% 100/3001 [02:34<1:10:51,  1.47s/it, _runtime=165, _timestamp=1.73e+9, loss=0.054, loss_mlm=0.0858, lr=0.002, norm_mask=8.02, norm_target=11.1]INFO:__main__:STEP 100 Running validation... 
 Generating 7 images with prompt: ['a <dog6> dog in the jungle', 'a <dog6> dog with a city in the background', 'a <dog6> dog with a mountain in the background', 'a <dog6> dog on top of a purple rug in a forest', 'a <dog6> dog in a chef outfit', 'a <dog6> dog in a police outfit', 'a cube shaped <dog6> dog'].

Generated


----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Step		|100
Raw		|a          photo      of         one        <dog6>     dog        over       a          white      basketball
Masked		|a          photo      of         one        <dog6>     dog        over       a          white      basketball
Preds		|a          photo      of         one        dog        dog        over       a          white      basketball
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

  0% 0/25 [00:00<?, ?it/s][A
  4% 1/25 [00:00<00:07,  3.10it/s][A
  8% 2/25 [00:00<00:07,  3.11it/s][A
 12% 3/25 [00:00<00:07,  3.13it/s][A
 16% 4/25 [00:01<00:06,  3.14it/s][A
 20% 5/25 [00:01<00:06,  3.14it/s][A
 24% 6/25 [00:01<00:06,  3.15it/s][A
 28% 7/25 [00:02<00:05,  3.15it/s][A
 32% 8/25 [00:02<00:05,  3.15it/s][A
 36% 9/25 [00:02<00:05,  3.15it/s][A
 40% 10/25 [00:03<00:04,  3.16it/s][A
 44% 11/25 [00:03<00:04,  3.15it/s][A
 48% 12/25 [00:03<00:04,  3.16it/s][A
 52% 13/25 [00:04<00:03,  3.16it/s][A
 56% 14/25 [00:04<00:03,  3.15it/s][A
 60% 15/25 [00:04<00:03,  3.15it/s][A
 64% 16/25 [00:05<00:02,  3.15it/s][A
 68% 17/25 [00:05<00:02,  3.15it/s][A
 72% 18/25 [00:05<00:02,  3.15it/s][A
 76% 19/25 [00:06<00:01,  3.15it/s][A
 80% 20/25 [00:06<00:01,  3.15it/s][A
 84% 21/25 [00:06<00:01,  3.15it/s][A
 88% 22/25 [00:06<00:00,  3.15it/s][A
 92% 23/25 [00:07<00:00,  3.15it/s][A
 96% 24/25 [00:07<00:00,  3.15it/s][A
100% 25/25 [00:07<00:00,  3.07it/s][A439510309 worker_seed
439510308 worker_seed
439510307 worker_seed
439510306 worker_seed
Traceback (most recent call last):
  File "/home/twkim/project/rich_context/textual_inversion/ti_train.py", line 1000, in <module>
    main()
  File "/home/twkim/project/rich_context/textual_inversion/ti_train.py", line 909, in main
    images,validation_prompts = log_validation(
  File "/home/twkim/project/rich_context/textual_inversion/ti_train.py", line 154, in log_validation
    images = pipeline(validation_prompts,
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py", line 786, in __call__
    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/utils/accelerate_utils.py", line 46, in wrapper
    return method(self, *args, **kwargs)
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/models/autoencoder_kl.py", line 264, in decode
    decoded = self._decode(z).sample
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/models/autoencoder_kl.py", line 251, in _decode
    dec = self.decoder(z)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/models/vae.py", line 270, in forward
    sample = up_block(sample, latent_embeds)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/models/unet_2d_blocks.py", line 2274, in forward
    hidden_states = resnet(hidden_states, temb=temb)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/twkim/project/rich_context/textual_inversion/./packages/diffusers/models/resnet.py", line 596, in forward
    hidden_states = self.norm1(hidden_states)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 279, in forward
    return F.group_norm(
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/torch/nn/functional.py", line 2558, in group_norm
    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacty of 23.69 GiB of which 1.32 GiB is free. Process 3387141 has 11.43 GiB memory in use. Including non-PyTorch memory, this process has 10.93 GiB memory in use. Of the allocated memory 9.37 GiB is allocated by PyTorch, and 822.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 3.036 MB of 3.036 MB uploaded (0.000 MB deduped)wandb: \ 3.036 MB of 3.036 MB uploaded (0.000 MB deduped)wandb: | 3.036 MB of 3.036 MB uploaded (0.000 MB deduped)wandb: / 3.036 MB of 3.055 MB uploaded (0.000 MB deduped)wandb: - 3.036 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: \ 3.036 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: | 3.049 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: / 3.066 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: - 3.066 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: \ 3.066 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: | 3.066 MB of 3.066 MB uploaded (0.000 MB deduped)wandb: / 3.066 MB of 3.066 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        loss ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ
wandb:    loss_mlm ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ
wandb:          lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   norm_mask ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: norm_target ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:        loss 0.05401
wandb:    loss_mlm 0.08576
wandb:          lr 0.002
wandb:   norm_mask 8.02037
wandb: norm_target 11.05378
wandb: 
wandb: Synced distinctive-blaze-592: https://wandb.ai/qlab-taewook/TI%20MLM%20SINGLE/runs/6jyabeze
wandb: Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240911_061910-6jyabeze/logs
Traceback (most recent call last):
  File "/home/twkim/anaconda3/envs/context/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/twkim/anaconda3/envs/context/bin/python', 'ti_train.py', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--train_data_dir1=/data/twkim/diffusion/personalization/collected/images/dog6', '--learnable_property=object', '--placeholder_token1=<dog6>', '--train_prior_concept1=dog', '--eval_prior_concept1=dog', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--max_train_steps=3001', '--learning_rate=5e-4', '--lr_scheduler=constant', '--initializer_token=dog', '--normalize_mask_embeds=0', '--lr_warmup_steps=0', '--output_dir=saved_models/ti_models/single_mtarget_capv7_prior_seed2940_rep1/dog6', '--seed=2940', '--mask_tokens=[MASK]', '--lambda_mlm=0.0005', '--freeze_mask_embedding=1', '--cls_net_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/cls_net_100000_ckpt.pt', '--mask_embed_path=saved_models/mlm_models/sd1_contextnetv4_nonpadding_1e4_unnorm_mprob015_batch150/checkpoints/checkpoint-100000/mask_embeds_100000_ckpt.pt', '--mask_prob=0.15', '--mlm_batch_size=25', '--scale_lr', '--eval_prompt_type=living', '--train_prompt_type=pet', '--silent=0', '--rev=0', '--check_tag=VERB-ADJ-ADV-PROPN-ADP-NOUN', '--mlm_target=masked', '--normalize_target1=0', '--caption_root=../datasets_pkgs/captions/v7', '--run_name=ti_cnetv4_prior_mlm00005_dog6_mprob015_mbatch25_mtarget_masked_tagged', '--report_to=wandb', '--project_name=TI MLM SINGLE', '--include_prior_concept=1']' returned non-zero exit status 1.
